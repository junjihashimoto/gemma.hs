{-# LANGUAGE OverloadedStrings #-}
{-# LANGUAGE ScopedTypeVariables #-}

module Gemma.Layers.DequantQ4GGUFSpec (spec) where

import Test.Hspec
import qualified Data.Vector.Storable as V
import Data.Vector.Storable (Vector)
import qualified Data.ByteString as BS
import Data.Word (Word8, Word16, Word32)
import Data.Bits ((.&.), shiftR, (.|.), shiftL)
import Foreign.Storable (Storable(..), peek, poke)
import Foreign.Ptr (Ptr, castPtr, plusPtr)
import Foreign.Marshal.Array (peekArray, pokeArray)
import qualified Foreign.Marshal.Alloc
import System.IO.Unsafe (unsafePerformIO)
import Graphics.WebGPU.Dawn.ContT (evalContT)
import Gemma.Layers.DequantQ4GGUF (runDequantQ4_0DSL, runMatmulQ4_0DSL)

-- Constants matching llama.cpp
qk4_0 :: Int
qk4_0 = 32  -- Block size for Q4_0

-- | Test spec for GGUF Q4_0 dequantization
--
-- This tests the core dequantization logic against the Python oracle.
-- The Python oracle (scripts/q4_oracle.py) generates ground truth data
-- matching llama.cpp's implementation.
spec :: Spec
spec = do
  describe "Q4_0 GGUF Dequantization" $ do
    it "dequantizes Q4_0 blocks correctly (Python oracle)" $ do
      -- Load test data generated by Python oracle
      weightsQ4 <- BS.readFile "test/q4_oracle_data/input_weights_q4.bin"
      weightsFP32Expected <- BS.readFile "test/q4_oracle_data/input_weights_f32.bin"
      metadataBS <- BS.readFile "test/q4_oracle_data/metadata.json"

      -- Parse metadata (simple approach)
      let nRows = 4  -- From q4_oracle.py (small test case)
          nCols = 64
          totalElements = nRows * nCols
          numBlocks = totalElements `div` qk4_0

      -- Verify Q4 data size: 18 bytes per block (2 bytes FP16 scale + 16 bytes nibbles)
      BS.length weightsQ4 `shouldBe` (numBlocks * 18)

      -- Dequantize using CPU reference implementation
      let weightsDeq = dequantizeQ4_0_CPU weightsQ4 totalElements

      -- Convert expected FP32 bytes to Vector Float
      let expectedWeights = bytesToFloatVector weightsFP32Expected totalElements

      -- Compare results (allow small quantization error)
      V.length weightsDeq `shouldBe` totalElements
      V.length expectedWeights `shouldBe` totalElements

      -- Calculate errors
      let errors = V.zipWith (\a b -> abs (a - b)) weightsDeq expectedWeights
          maxError = V.maximum errors
          meanError = V.sum errors / fromIntegral (V.length errors)

      -- These thresholds match the Python oracle's round-trip test
      maxError `shouldSatisfy` (< 0.04)   -- Max quantization error < 0.04
      meanError `shouldSatisfy` (< 0.01)  -- Mean quantization error < 0.01

      putStrLn $ "\n  Max error: " ++ show maxError
      putStrLn $ "  Mean error: " ++ show meanError

    it "dequantizes Q4_0 blocks on GPU correctly (Python oracle)" $ do
      -- Load test data
      weightsQ4 <- BS.readFile "test/q4_oracle_data/input_weights_q4.bin"
      weightsFP32Expected <- BS.readFile "test/q4_oracle_data/input_weights_f32.bin"

      let nRows = 4
          nCols = 64
          totalElements = nRows * nCols

      -- Convert Q4 ByteString to Word32 array for GPU
      let q4Word32 = byteStringToWord32Vector weightsQ4

      -- Run GPU dequantization
      weightsDeqGPU <- evalContT $ runDequantQ4_0DSL q4Word32 totalElements

      -- Convert expected FP32 bytes to Vector Float
      let expectedWeights = bytesToFloatVector weightsFP32Expected totalElements

      -- Compare GPU results with expected
      V.length weightsDeqGPU `shouldBe` totalElements
      V.length expectedWeights `shouldBe` totalElements

      let errors = V.zipWith (\a b -> abs (a - b)) weightsDeqGPU expectedWeights
          maxError = V.maximum errors
          meanError = V.sum errors / fromIntegral (V.length errors)

      -- GPU should match within similar tolerance as CPU
      maxError `shouldSatisfy` (< 0.04)
      meanError `shouldSatisfy` (< 0.01)

      putStrLn $ "\n  GPU Max error: " ++ show maxError
      putStrLn $ "  GPU Mean error: " ++ show meanError

    it "performs Q4_0 matrix-vector multiply on GPU (Python oracle)" $ do
      -- Load test data
      weightsQ4 <- BS.readFile "test/q4_oracle_data/input_weights_q4.bin"
      inputVec <- BS.readFile "test/q4_oracle_data/input_vector_f32.bin"
      expectedOutput <- BS.readFile "test/q4_oracle_data/expected_output_f32.bin"

      let nRows = 4
          nCols = 64

      -- Convert to GPU format
      let q4Word32 = byteStringToWord32Vector weightsQ4
          inputVecF32 = bytesToFloatVector inputVec nCols
          expectedF32 = bytesToFloatVector expectedOutput nRows

      -- Run GPU matrix-vector multiply (without soft-cap)
      resultGPU <- evalContT $ runMatmulQ4_0DSL q4Word32 inputVecF32 nRows nCols False 30.0

      -- Compare with expected
      V.length resultGPU `shouldBe` nRows

      let errors = V.zipWith (\a b -> abs (a - b)) resultGPU expectedF32
          maxError = V.maximum errors
          meanError = V.sum errors / fromIntegral (V.length errors)

      -- Matrix-vector should match within Q4 tolerance
      maxError `shouldSatisfy` (< 0.05)
      meanError `shouldSatisfy` (< 0.02)

      putStrLn $ "\n  Matmul GPU Max error: " ++ show maxError
      putStrLn $ "  Matmul GPU Mean error: " ++ show meanError

    it "applies Gemma 3 soft-capping correctly (Python oracle)" $ do
      -- Load test data
      weightsQ4 <- BS.readFile "test/q4_oracle_data/input_weights_q4.bin"
      inputVec <- BS.readFile "test/q4_oracle_data/input_vector_f32.bin"
      expectedOutputSoftCap <- BS.readFile "test/q4_oracle_data/expected_output_softcap_f32.bin"

      let nRows = 4
          nCols = 64

      -- Convert to GPU format
      let q4Word32 = byteStringToWord32Vector weightsQ4
          inputVecF32 = bytesToFloatVector inputVec nCols
          expectedF32 = bytesToFloatVector expectedOutputSoftCap nRows

      -- Run GPU matrix-vector multiply (WITH soft-cap)
      resultGPU <- evalContT $ runMatmulQ4_0DSL q4Word32 inputVecF32 nRows nCols True 30.0

      -- Compare with expected
      V.length resultGPU `shouldBe` nRows

      let errors = V.zipWith (\a b -> abs (a - b)) resultGPU expectedF32
          maxError = V.maximum errors
          meanError = V.sum errors / fromIntegral (V.length errors)

      -- Soft-cap should match within Q4 tolerance
      maxError `shouldSatisfy` (< 0.05)
      meanError `shouldSatisfy` (< 0.02)

      putStrLn $ "\n  Soft-cap GPU Max error: " ++ show maxError
      putStrLn $ "  Soft-cap GPU Mean error: " ++ show meanError

-- | CPU reference implementation of Q4_0 dequantization
--
-- This matches llama.cpp's dequantize_row_q4_0 exactly:
-- 1. Read 2-byte FP16 scale
-- 2. Read 16 bytes of packed nibbles (2 nibbles per byte)
-- 3. Dequantize: weight = (nibble - 8) * scale
dequantizeQ4_0_CPU :: BS.ByteString -> Int -> Vector Float
dequantizeQ4_0_CPU bs totalElements = unsafePerformIO $ do
  let numBlocks = totalElements `div` qk4_0

  -- Allocate output buffer
  let output = V.generate totalElements $ \idx ->
        let blockIdx = idx `div` qk4_0
            posInBlock = idx `mod` qk4_0
            blockOffset = blockIdx * 18  -- 18 bytes per block

            -- Read FP16 scale (2 bytes, little-endian)
            scaleByte0 = fromIntegral $ BS.index bs blockOffset
            scaleByte1 = fromIntegral $ BS.index bs (blockOffset + 1)
            scaleU16 = scaleByte0 .|. (scaleByte1 `shiftL` 8)
            scale = fp16BitsToFP32 scaleU16

            -- Read nibble from packed bytes (16 bytes starting at offset+2)
            -- Nibbles are laid out: [0-15 in low nibbles, 16-31 in high nibbles]
            nibbleByteIdx = if posInBlock < 16
                            then blockOffset + 2 + posInBlock
                            else blockOffset + 2 + (posInBlock - 16)
            nibbleByte = BS.index bs nibbleByteIdx
            nibble = if posInBlock < 16
                     then fromIntegral (nibbleByte .&. 0x0F)  -- Low nibble
                     else fromIntegral (nibbleByte `shiftR` 4)  -- High nibble

            -- Dequantize: (nibble - 8) * scale
            weight = (nibble - 8) * scale
        in weight

  return output

-- | Convert FP16 raw bits to FP32 (matching Python's fp16_bits_to_fp32)
--
-- This implements IEEE 754 half-precision to single-precision conversion.
-- Pure Haskell implementation without unsafe IO for performance.
fp16BitsToFP32 :: Word16 -> Float
fp16BitsToFP32 bits =
  let sign = (bits `shiftR` 15) .&. 1
      exponent = (bits `shiftR` 10) .&. 0x1F
      mantissa = bits .&. 0x3FF

      -- Convert to FP32 by reinterpreting bits
      fp32Bits = if exponent == 0 then
                   -- Subnormal or zero
                   if mantissa == 0
                     then (fromIntegral sign `shiftL` 31) :: Word32  -- Zero
                     else error "Subnormal FP16 not yet supported"  -- Rare case
                 else if exponent == 0x1F then
                   -- Infinity or NaN
                   let expBits = (0xFF :: Word32) `shiftL` 23
                       mantBits = (fromIntegral mantissa :: Word32) `shiftL` 13
                   in (fromIntegral sign `shiftL` 31) .|. expBits .|. mantBits
                 else
                   -- Normalized number
                   let exp32 = (fromIntegral exponent - 15 + 127) :: Word32
                       expBits = exp32 `shiftL` 23
                       mantBits = (fromIntegral mantissa :: Word32) `shiftL` 13
                   in (fromIntegral sign `shiftL` 31) .|. expBits .|. mantBits

  -- Convert Word32 bits to Float using unsafePerformIO (once per call)
  in unsafePerformIO $ Foreign.Marshal.Alloc.alloca $ \(ptr :: Ptr Word32) -> do
       poke ptr fp32Bits
       peek (castPtr ptr :: Ptr Float)

-- | Convert ByteString to Vector Float (little-endian)
bytesToFloatVector :: BS.ByteString -> Int -> Vector Float
bytesToFloatVector bs n = unsafePerformIO $ do
  let bytes = BS.unpack bs
  V.generateM n $ \i -> do
    let offset = i * 4
        b0 = fromIntegral $ bytes !! offset
        b1 = fromIntegral $ bytes !! (offset + 1)
        b2 = fromIntegral $ bytes !! (offset + 2)
        b3 = fromIntegral $ bytes !! (offset + 3)
        bits = b0 .|. (b1 `shiftL` 8) .|. (b2 `shiftL` 16) .|. (b3 `shiftL` 24)
    Foreign.Marshal.Alloc.alloca $ \ptr -> do
      poke (castPtr ptr :: Ptr Word32) bits
      peek (ptr :: Ptr Float)

-- | Convert ByteString to Vector Word32 (little-endian, for GPU upload)
byteStringToWord32Vector :: BS.ByteString -> Vector Word32
byteStringToWord32Vector bs = unsafePerformIO $ do
  let bytes = BS.unpack bs
      numWord32 = (length bytes + 3) `div` 4  -- Round up to Word32 boundary
  V.generateM numWord32 $ \i -> do
    let offset = i * 4
        -- Handle partial last word by padding with zeros
        b0 = if offset < length bytes then fromIntegral (bytes !! offset) else 0
        b1 = if offset + 1 < length bytes then fromIntegral (bytes !! (offset + 1)) else 0
        b2 = if offset + 2 < length bytes then fromIntegral (bytes !! (offset + 2)) else 0
        b3 = if offset + 3 < length bytes then fromIntegral (bytes !! (offset + 3)) else 0
        word32 = b0 .|. (b1 `shiftL` 8) .|. (b2 `shiftL` 16) .|. (b3 `shiftL` 24)
    return word32
